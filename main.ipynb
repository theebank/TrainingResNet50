{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theebankumaresan/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/theebankumaresan/anaconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: <E03EDA44-89AE-3115-9796-62BA9E0E2EDE> /Users/theebankumaresan/anaconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <F2FE5CF8-5B5B-3FAD-ADF8-C77D90F49FC9> /Users/theebankumaresan/anaconda3/lib/python3.11/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customCOCODataset(Dataset):\n",
    "    def __init__(self, data_dir, annotations_file, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.annotations_file = annotations_file\n",
    "        self.transform = transform\n",
    "        self.image_list = self.load_image_list()\n",
    "        self.annotations = self.load_annotations()\n",
    "\n",
    "    def load_image_list(self):\n",
    "        image_list = []\n",
    "        for root, _, files in os.walk(self.data_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.jpg'):\n",
    "                    image_list.append(os.path.join(root, file))\n",
    "        return image_list\n",
    "\n",
    "    def load_annotations(self):\n",
    "        with open(self.annotations_file, 'r') as f:\n",
    "            coco_data = json.load(f)\n",
    "\n",
    "        \n",
    "        annotations = {}\n",
    "        for annotation in coco_data['annotations']:\n",
    "            img_filename = os.path.splitext(os.path.basename(coco_data['images'][annotation['image_id']]['file_name']))[0]\n",
    "            if img_filename not in annotations:\n",
    "                annotations[img_filename] = []\n",
    "            annotations[img_filename].append(annotation)\n",
    "\n",
    "        return annotations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_list[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        img_filename = os.path.splitext(os.path.basename(img_path))[0]  \n",
    "\n",
    "        annotations = self.annotations.get(img_filename, []) \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, annotations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the Pretrained resnet model and freezing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resNet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "for param in resNet.parameters():\n",
    "    param.requires_grad = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RetrievalModel, self).__init__()\n",
    "        self.features = nn.Sequential(*list(resNet.children())[:-1])\n",
    "        self.fc = nn.Linear(2048, 256) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "retrievalModel = RetrievalModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTransform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = 'my_data/train'\n",
    "annotationsFile = 'my_data/my_train_coco.json'\n",
    "custom_dataset = customCOCODataset(dataDir, annotationsFile, transform=dataTransform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLoader = DataLoader(custom_dataset, batch_size=32, shuffle=True)\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "optimizer = torch.optim.Adam(retrievalModel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Loss: 0.6588\n",
      "Epoch [2/5] Loss: 0.3470\n",
      "Epoch [3/5] Loss: 0.0879\n",
      "Epoch [4/5] Loss: 0.0775\n",
      "Epoch [5/5] Loss: 0.0516\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for images, _ in dataLoader:\n",
    "        anchor_images, positive_images = torch.chunk(images, 2, dim=0) \n",
    "        anchor_feat = retrievalModel(anchor_images)\n",
    "        positive_feat = retrievalModel(positive_images)\n",
    "\n",
    "        target = torch.ones(anchor_images.size(0))\n",
    "\n",
    "        loss = criterion(anchor_feat, positive_feat, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# torch.save(retrievalModel.state_dict(), 'retrieval_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
